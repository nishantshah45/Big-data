{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVs9nmwnzq3XI5xiDu8XFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishantshah45/Big-data/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdn24nG2m8T6",
        "outputId": "f2c14fd0-3923-4382-bae5-335b3a549169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"WordCount App\")\n",
        "\n",
        "rdd=sc.textFile(\"sample.txt\")\n",
        "\n",
        "word_counts=(\n",
        "    rdd.flatMap(lambda line: line.split())\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        ")\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "    sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6IP6oRKooum",
        "outputId": "6ff17829-eba3-4ee9-c2dc-ec05c954adf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN: 1\n",
            "—: 1\n",
            "Scientists: 2\n",
            "are: 8\n",
            "trying: 1\n",
            "to: 10\n",
            "solve: 2\n",
            "a: 4\n",
            "decade-long: 1\n",
            "mystery: 1\n",
            "by: 5\n",
            "determining: 1\n",
            "the: 29\n",
            "identity: 1\n",
            "of: 13\n",
            "anomalous: 5\n",
            "signals: 5\n",
            "detected: 1\n",
            "from: 3\n",
            "below: 2\n",
            "ice: 2\n",
            "in: 7\n",
            "Antarctica.: 1\n",
            "The: 7\n",
            "strange: 1\n",
            "radio: 3\n",
            "waves: 3\n",
            "emerged: 1\n",
            "during: 2\n",
            "search: 4\n",
            "for: 4\n",
            "another: 1\n",
            "unusual: 1\n",
            "phenomenon:: 1\n",
            "high-energy: 1\n",
            "cosmic: 5\n",
            "particles: 3\n",
            "known: 1\n",
            "as: 1\n",
            "neutrinos.: 2\n",
            "Arriving: 1\n",
            "at: 2\n",
            "Earth: 2\n",
            "far: 1\n",
            "reaches: 1\n",
            "cosmos,: 1\n",
            "neutrinos: 5\n",
            "often: 1\n",
            "called: 1\n",
            "“ghostly”: 1\n",
            "because: 3\n",
            "they: 4\n",
            "extremely: 1\n",
            "volatile,: 1\n",
            "or: 4\n",
            "vaporous,: 1\n",
            "and: 11\n",
            "can: 2\n",
            "go: 1\n",
            "through: 3\n",
            "any: 1\n",
            "kind: 1\n",
            "matter: 1\n",
            "without: 1\n",
            "changing.: 1\n",
            "Over: 1\n",
            "past: 1\n",
            "decade,: 1\n",
            "researchers: 2\n",
            "have: 5\n",
            "conducted: 2\n",
            "multiple: 1\n",
            "experiments: 1\n",
            "using: 1\n",
            "vast: 1\n",
            "expanses: 1\n",
            "water: 2\n",
            "that: 7\n",
            "designed: 1\n",
            "neutrinos,: 1\n",
            "which: 3\n",
            "could: 3\n",
            "shed: 1\n",
            "light: 1\n",
            "on: 2\n",
            "mysterious: 1\n",
            "rays,: 1\n",
            "most: 3\n",
            "highly: 2\n",
            "energetic: 2\n",
            "universe.: 1\n",
            "One: 1\n",
            "these: 2\n",
            "projects: 1\n",
            "was: 2\n",
            "NASA’s: 1\n",
            "Antarctic: 1\n",
            "Impulsive: 1\n",
            "Transient: 1\n",
            "Antenna,: 1\n",
            "ANITA,: 1\n",
            "experiment,: 1\n",
            "flew: 1\n",
            "balloons: 1\n",
            "carrying: 1\n",
            "instruments: 1\n",
            "above: 1\n",
            "Antarctica: 1\n",
            "between: 1\n",
            "2006: 1\n",
            "2016.: 1\n",
            "It: 1\n",
            "this: 1\n",
            "hunt: 1\n",
            "ANITA: 2\n",
            "picked: 1\n",
            "up: 2\n",
            "didn’t: 1\n",
            "seem: 1\n",
            "be: 3\n",
            "came: 1\n",
            "horizon,: 1\n",
            "suggesting: 1\n",
            "had: 1\n",
            "passed: 1\n",
            "thousands: 1\n",
            "miles: 1\n",
            "rock: 1\n",
            "before: 1\n",
            "reaching: 1\n",
            "detector.: 1\n",
            "But: 2\n",
            "should: 1\n",
            "been: 3\n",
            "absorbed: 1\n",
            "rock.: 1\n",
            "team: 1\n",
            "believed: 1\n",
            "not: 4\n",
            "explained: 1\n",
            "current: 1\n",
            "understanding: 1\n",
            "particle: 2\n",
            "physics.: 1\n",
            "10.-Credit-Simonelli-Andrea_INFN.jpg: 1\n",
            "Related: 1\n",
            "article: 1\n",
            "detect: 1\n",
            "record-breaking: 1\n",
            "‘ghost: 1\n",
            "particle’: 1\n",
            "Mediterranean: 1\n",
            "Sea: 1\n",
            "Follow-up: 1\n",
            "observations: 1\n",
            "analyses: 1\n",
            "with: 2\n",
            "other: 1\n",
            "instruments,: 1\n",
            "including: 1\n",
            "one: 1\n",
            "recently: 1\n",
            "Pierre: 3\n",
            "Auger: 3\n",
            "Observatory: 1\n",
            "Argentina,: 1\n",
            "able: 2\n",
            "find: 2\n",
            "same: 1\n",
            "signals.: 1\n",
            "results: 1\n",
            "Collaboration: 1\n",
            "were: 2\n",
            "published: 1\n",
            "journal: 1\n",
            "Physical: 1\n",
            "Review: 1\n",
            "Letters: 1\n",
            "March.: 1\n",
            "origin: 1\n",
            "remains: 1\n",
            "unclear,: 1\n",
            "said: 1\n",
            "study: 2\n",
            "coauthor: 1\n",
            "Stephanie: 1\n",
            "Wissel,: 1\n",
            "associate: 1\n",
            "professor: 1\n",
            "physics,: 2\n",
            "astronomy: 1\n",
            "astrophysics: 1\n",
            "Pennsylvania: 1\n",
            "State: 1\n",
            "University.: 1\n",
            "“Our: 1\n",
            "new: 2\n",
            "indicates: 1\n",
            "such: 2\n",
            "(signals): 1\n",
            "seen: 1\n",
            "an: 1\n",
            "experiment: 1\n",
            "…: 1\n",
            "like: 2\n",
            "Observatory,”: 1\n",
            "Wissel: 1\n",
            "said.: 1\n",
            "“So,: 1\n",
            "it: 2\n",
            "does: 1\n",
            "indicate: 1\n",
            "there: 1\n",
            "is: 2\n",
            "but: 1\n",
            "rather: 1\n",
            "more: 2\n",
            "information: 1\n",
            "add: 1\n",
            "story.”: 1\n",
            "Larger,: 1\n",
            "sensitive: 1\n",
            "detectors: 1\n",
            "may: 1\n",
            "mystery,: 1\n",
            "ultimately: 1\n",
            "prove: 1\n",
            "whether: 1\n",
            "fluke,: 1\n",
            "while: 1\n",
            "continuing: 1\n",
            "enigmatic: 1\n",
            "their: 2\n",
            "sources,: 2\n",
            "scientists: 2\n",
            "say.: 1\n",
            "Detecting: 1\n",
            "allows: 1\n",
            "trace: 1\n",
            "them: 3\n",
            "back: 1\n",
            "believe: 1\n",
            "primarily: 1\n",
            "rays: 3\n",
            "strike: 1\n",
            "our: 1\n",
            "planet’s: 1\n",
            "atmosphere.: 1\n",
            "universe,: 1\n",
            "made: 1\n",
            "mostly: 1\n",
            "protons: 1\n",
            "atomic: 1\n",
            "nuclei,: 1\n",
            "unleashed: 1\n",
            "across: 2\n",
            "universe: 1\n",
            "whatever: 1\n",
            "produces: 1\n",
            "powerful: 1\n",
            "accelerator: 1\n",
            "dwarfs: 1\n",
            "capabilities: 1\n",
            "Large: 1\n",
            "Hadron: 1\n",
            "Collider.: 1\n",
            "Neutrinos: 1\n",
            "help: 1\n",
            "astronomers: 1\n",
            "better: 1\n",
            "understand: 1\n",
            "what: 1\n",
            "launches: 1\n",
            "cosmos.: 1\n",
            "difficult: 1\n",
            "almost: 1\n",
            "no: 1\n",
            "mass: 1\n",
            "pass: 1\n",
            "extreme: 1\n",
            "environments,: 1\n",
            "stars: 1\n",
            "entire: 1\n",
            "galaxies,: 1\n",
            "unchanged.: 1\n",
            "They: 1\n",
            "do,: 1\n",
            "however,: 1\n",
            "interact: 1\n",
            "ice.: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "data =[\n",
        "    \"apple orange banana\",\n",
        "    \"banana orange mango\",\n",
        "    \"apple orange banana mango\"\n",
        "]\n",
        "def mapper(lines):\n",
        "  mapped=[]\n",
        "  for line in lines:\n",
        "      for word in line.split():\n",
        "          mapped.append((word,1))\n",
        "  return mapped\n",
        "\n",
        "def shuffle(mapped_data):\n",
        "   grouped =defaultdict(list)\n",
        "   for key, value in mapped_data:\n",
        "        grouped[key].append(value)\n",
        "   return grouped\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def reducer(grouped):\n",
        "    reduced={}\n",
        "    for key, values in grouped.items():\n",
        "      reduced[key] = sum(values)        # ✅ Correct assignment for a dictionary\n",
        "\n",
        "    return reduced\n",
        "\n",
        "mapped_data=mapper(data)\n",
        "grouped_data=shuffle(mapped_data)\n",
        "reduced_data=reducer(grouped_data)\n",
        "\n",
        "for word, count in reduced_data.items():  # ✅ Proper way to unpack key-value pairs\n",
        "\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceC-DiUGuj0M",
        "outputId": "534db649-9ee9-4e4f-f9f5-7e462f2ee93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple: 2\n",
            "orange: 3\n",
            "banana: 3\n",
            "mango: 2\n"
          ]
        }
      ]
    }
  ]
}